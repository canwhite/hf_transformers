'''
ä½¿ç”¨AutoClassåŠ è½½é¢„è®­ç»ƒå®ä¾‹
from_pretrained()æ–¹æ³•å…è®¸æ‚¨å¿«é€ŸåŠ è½½ä»»ä½•æ¶æ„çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå› æ­¤æ‚¨ä¸å¿…èŠ±è´¹æ—¶é—´å’Œç²¾åŠ›ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚

åœ¨è¿™ä¸ªæ•™ç¨‹ä¸­ï¼Œå­¦ä¹ å¦‚ä½•ï¼š
åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨ï¼ˆtokenizerï¼‰
åŠ è½½é¢„è®­ç»ƒçš„å›¾åƒå¤„ç†å™¨(image processor)
åŠ è½½é¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨(feature extractor)
åŠ è½½é¢„è®­ç»ƒçš„å¤„ç†å™¨(processor)
åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹ã€‚
'''

from transformers import pipeline



'''===================AutoTokenizer:åˆ†è¯===================='''
from transformers import AutoTokenizer
'''
# è®¾ç½®æŒ‰å¥åˆ†
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased", do_basic_tokenize=False)
tokenizer.add_special_tokens({'sep_token': '[SEP]'})
tokenizer.add_special_tokens({'cls_token': '[CLS]'})
tokenizer.add_special_tokens({'pad_token': '[PAD]'})
tokenizer.add_special_tokens({'unk_token': '[UNK]'})
tokenizer.add_special_tokens({'mask_token': '[MASK]'})

def tokenize_by_sentence(text):
    sentences = text.split('. ')
    tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in sentences]
    return tokenized_sentences

sequence = "In a hole in the ground there lived a hobbit."
tokenized_sequence = tokenize_by_sentence(sequence)
print(tokenized_sequence)
'''

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

sequence = "In a hole in the ground there lived a hobbit."
print(tokenizer(sequence))



'''===================AutoImageProcessor:è§†è§‰ä»»åŠ¡==================='''
from transformers import AutoImageProcessor
'''
image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
'''


'''===================AutoFeatureExtractor:éŸ³é¢‘ä»»åŠ¡================='''
from transformers import AutoFeatureExtractor
'''
feature_extractor = AutoFeatureExtractor.from_pretrained(
    "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
)
'''

'''==========================AutoProcessor:å¤šæ¨¡æ€========================'''
# å¤šæ¨¡æ€ä»»åŠ¡éœ€è¦ä¸€ç§processorï¼Œå°†ä¸¤ç§ç±»å‹çš„é¢„å¤„ç†å·¥å…·ç»“åˆèµ·æ¥ã€‚
# ä¾‹å¦‚ï¼ŒLayoutLMV2æ¨¡å‹éœ€è¦ä¸€ä¸ªimage processoæ¥å¤„ç†å›¾åƒå’Œä¸€ä¸ªtokenizeræ¥å¤„ç†æ–‡æœ¬ï¼›processorå°†ä¸¤è€…ç»“åˆèµ·æ¥
from transformers import AutoProcessor

'''
# åŠ è½½å›¾åƒå¤„ç†å™¨
image_processor = AutoImageProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("microsoft/layoutlmv2-base-uncased")

# ç»“åˆå›¾åƒå¤„ç†å™¨å’Œåˆ†è¯å™¨
processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")

# ç¤ºä¾‹ï¼šå¤„ç†å›¾åƒå’Œæ–‡æœ¬
image = "path_to_image.jpg"
text = "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ–‡æœ¬"

# ä½¿ç”¨processorå¤„ç†å›¾åƒå’Œæ–‡æœ¬
processed_inputs = processor(images=image, text=text, return_tensors="pt")

# æ‰“å°å¤„ç†åçš„è¾“å…¥
print(processed_inputs)
'''


'''==========================AutoModel========================'''
from transformers import AutoModel
from transformers import AutoModelForSequenceClassification #åŠ è½½ç”¨äºåºåˆ—åˆ†ç±»çš„æ¨¡å‹
from transformers import AutoModelForTokenClassification #åŠ è½½ç”¨äºæ ‡è®°çš„æ¨¡å‹
'''
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
model = AutoModelForTokenClassification.from_pretrained("distilbert-base-uncased")
'''

# ä¸Šè¾¹è¿™äº›æœ‰äº›æ˜¯ä¸ºäº†æ¨ç†ï¼Œæœ‰äº›æ˜¯ä¸ºäº†è®­ç»ƒï¼Œæˆ‘ä»¬è¿™é‡Œé€‰ä¸ºäº†æ¨ç†ï¼Œsoæ˜¯å¯ä»¥çµæ´»åº”ç”¨çš„ï¼Œç‚¹é¢˜äº†
pt_model = AutoModelForSequenceClassification.from_pretrained("/Users/zack/Desktop/hf_transformers/pt_save_pretrained")

classifier = pipeline("sentiment-analysis", model=pt_model, tokenizer=tokenizer)
result =  classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ğŸ¤— Transformers.")
print(result)