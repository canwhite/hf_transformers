'''
é¢„å¤„ç†æ•°æ®
åœ¨æ‚¨å¯ä»¥åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæ•°æ®éœ€è¦è¢«é¢„å¤„ç†ä¸ºæœŸæœ›çš„æ¨¡å‹è¾“å…¥æ ¼å¼ã€‚
æ— è®ºæ‚¨çš„æ•°æ®æ˜¯æ–‡æœ¬ã€å›¾åƒè¿˜æ˜¯éŸ³é¢‘ï¼Œå®ƒä»¬éƒ½éœ€è¦è¢«è½¬æ¢å¹¶ç»„åˆæˆæ‰¹é‡çš„å¼ é‡ã€‚
ğŸ¤— Transformers æä¾›äº†ä¸€ç»„é¢„å¤„ç†ç±»æ¥å¸®åŠ©å‡†å¤‡æ•°æ®ä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†äº†è§£ä»¥ä¸‹å†…å®¹ï¼š
1ï¼‰å¯¹äºæ–‡æœ¬ï¼Œä½¿ç”¨åˆ†è¯å™¨(Tokenizer)å°†æ–‡æœ¬è½¬æ¢ä¸ºä¸€ç³»åˆ—æ ‡è®°(tokens)ï¼Œå¹¶åˆ›å»ºtokensçš„æ•°å­—è¡¨ç¤ºï¼Œå°†å®ƒä»¬ç»„åˆæˆå¼ é‡ã€‚
2) å¯¹äºè¯­éŸ³å’ŒéŸ³é¢‘ï¼Œä½¿ç”¨ç‰¹å¾æå–å™¨(Feature extractor)ä»éŸ³é¢‘æ³¢å½¢ä¸­æå–é¡ºåºç‰¹å¾å¹¶å°†å…¶è½¬æ¢ä¸ºå¼ é‡ã€‚
3) å›¾åƒè¾“å…¥ä½¿ç”¨å›¾åƒå¤„ç†å™¨(ImageProcessor)å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡ã€‚
4) å¤šæ¨¡æ€è¾“å…¥ï¼Œä½¿ç”¨å¤„ç†å™¨(Processor)ç»“åˆäº†Tokenizerå’ŒImageProcessoræˆ–Processorã€‚
'''

'''
==============================================1.æ–‡æœ¬=======================================================
å¯¹äºæ–‡æœ¬ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨Tokenizerç±»å°†æ–‡æœ¬è½¬æ¢ä¸ºä¸€ç³»åˆ—æ ‡è®°(tokens)ï¼Œå¹¶åˆ›å»ºtokensçš„æ•°å­—è¡¨ç¤ºï¼Œå°†å®ƒä»¬ç»„åˆæˆå¼ é‡ã€‚
'''

'''1)åŸºæœ¬ä½¿ç”¨'''
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
print(encoded_input)
'''
input_ids æ˜¯ä¸å¥å­ä¸­æ¯ä¸ªtokenå¯¹åº”çš„ç´¢å¼•ã€‚
attention_mask æŒ‡ç¤ºæ˜¯å¦åº”è¯¥å…³æ³¨ä¸€ä¸ªtoeknã€‚
token_type_ids åœ¨å­˜åœ¨å¤šä¸ªåºåˆ—æ—¶æ ‡è¯†ä¸€ä¸ªtokenå±äºå“ªä¸ªåºåˆ—ã€‚
'''
# {'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102],
#  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
#  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}

input_ids = encoded_input["input_ids"]
#ä¹Ÿé€šè¿‡è§£ç  input_ids æ¥è¿”å›æ‚¨çš„è¾“å…¥ï¼š
decoderInfo =  tokenizer.decode(input_ids)
print("----",decoderInfo)



'''
2ï¼‰å¦‚æœæœ‰å¤šä¸ªå¥å­éœ€è¦é¢„å¤„ç†ï¼Œå°†å®ƒä»¬ä½œä¸ºåˆ—è¡¨ä¼ é€’ç»™tokenizerï¼š
'''

batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_inputs = tokenizer(batch_sentences)
print(encoded_inputs)

'''
3ï¼‰å¡«å……ã€æˆªæ–­å’Œæ„å»ºå¼ é‡ï¼š
3-1ï¼‰å¡«å……
å¥å­çš„é•¿åº¦å¹¶ä¸æ€»æ˜¯ç›¸åŒï¼Œè¿™å¯èƒ½ä¼šæˆä¸ºä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºæ¨¡å‹è¾“å…¥çš„å¼ é‡éœ€è¦å…·æœ‰ç»Ÿä¸€çš„å½¢çŠ¶ã€‚
å¡«å……æ˜¯ä¸€ç§ç­–ç•¥ï¼Œé€šè¿‡åœ¨è¾ƒçŸ­çš„å¥å­ä¸­æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„padding tokenï¼Œä»¥ç¡®ä¿å¼ é‡æ˜¯çŸ©å½¢çš„ã€‚

3-2ï¼‰æˆªæ–­
å¦ä¸€æ–¹é¢ï¼Œæœ‰æ—¶å€™ä¸€ä¸ªåºåˆ—å¯èƒ½å¯¹æ¨¡å‹æ¥è¯´å¤ªé•¿äº†ã€‚
åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦å°†åºåˆ—æˆªæ–­ä¸ºæ›´çŸ­çš„é•¿åº¦ã€‚
å°† truncation å‚æ•°è®¾ç½®ä¸º Trueï¼Œä»¥å°†åºåˆ—æˆªæ–­ä¸ºæ¨¡å‹æ¥å—çš„æœ€å¤§é•¿åº¦ï¼š

3-3)æ„å»ºå¼ é‡
å°† return_tensors å‚æ•°è®¾ç½®ä¸º ptï¼ˆå¯¹äºPyTorchï¼‰æˆ– tfï¼ˆå¯¹äºTensorFlowï¼‰ï¼š

'''

batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True ,truncation=True,return_tensors="pt")
print(encoded_input)



'''
==============================================2.éŸ³é¢‘=======================================================
å¯¹äºæ–‡æœ¬ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨Tokenizerç±»å°†æ–‡æœ¬è½¬æ¢ä¸ºä¸€ç³»åˆ—æ ‡è®°(tokens)ï¼Œå¹¶åˆ›å»ºtokensçš„æ•°å­—è¡¨ç¤ºï¼Œå°†å®ƒä»¬ç»„åˆæˆå¼ é‡ã€‚
'''


