import torch
from jssyntax import List,Set,Map
from torch import nn
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForSequenceClassification
# from transformers import TFAutoModelForSequenceClassification

''' ================section1: pipeline==============='''

''' 1)åŸºç¡€ä½¿ç”¨'''
#pipeline() ä¼šä¸‹è½½å¹¶ç¼“å­˜ä¸€ä¸ªç”¨äºæƒ…æ„Ÿåˆ†æçš„é»˜è®¤çš„é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ã€‚ç°åœ¨ä½ å¯ä»¥åœ¨ç›®æ ‡æ–‡æœ¬ä¸Šä½¿ç”¨ classifier äº†ï¼š
classifier = pipeline("sentiment-analysis",device = 0)
#å¦‚æœä¸æ­¢ä¸€ä¸ªè¾“å…¥ï¼Œå¯ä»¥å°†è¾“å…¥ä½œä¸ºåˆ—è¡¨ä¼ å…¥
result =  classifier("We are very happy to show you the ğŸ¤— Transformers library.")

''' 2)åˆ¶å®šæ¨¡å‹å’Œåˆ†è¯å™¨ '''
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
# åŠ è½½æ¨¡å‹
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
# è·å–åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(model_name)


classifier = pipeline("sentiment-analysis", model=pt_model, tokenizer=tokenizer)
result =  classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ğŸ¤— Transformers.")

''' ================section2: autoClass ä»¥åŠæƒ…æ„Ÿåˆ†æè¿‡ç¨‹==============='''

''' 1)åˆ†è¯å™¨'''
encoding = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
print(encoding)
# input_idsï¼šç”¨æ•°å­—è¡¨ç¤ºçš„ tokenã€‚
# attention_maskï¼šåº”è¯¥å…³æ³¨å“ªäº› token çš„æŒ‡ç¤ºã€‚
# {'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], 
# 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
# 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}

#åˆ†è¯å™¨ä¹Ÿå¯ä»¥æ¥å—åˆ—è¡¨ä½œä¸ºè¾“å…¥ï¼Œ
#å¹¶å¡«å……å’Œæˆªæ–­æ–‡æœ¬ï¼Œ
#è¿”å›å…·æœ‰ç»Ÿä¸€é•¿åº¦çš„æ‰¹æ¬¡ï¼š
pt_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)
print(pt_batch)

''' 2)model
# å°†åˆ†è¯åçš„è¾“å…¥æ‰¹æ¬¡ä¼ é€’ç»™æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚
æ¨¡å‹è¾“å‡º logitsï¼Œé€šè¿‡ softmax å‡½æ•°è½¬æ¢ä¸ºæ¦‚ç‡ã€‚
'''

pt_outputs = pt_model(**pt_batch)
# æ¨¡å‹åœ¨ logits å±æ€§è¾“å‡ºæœ€ç»ˆçš„æ¿€æ´»ç»“æœ. åœ¨ logits ä¸Šåº”ç”¨ softmax å‡½æ•°æ¥æŸ¥è¯¢æ¦‚ç‡:
pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)


''' 3)ä¿å­˜æ¨¡å‹
ä¸‹æ¬¡å¯ä»¥ç›´æ¥ä»æœ¬åœ°åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œé¿å…é‡å¤ä¸‹è½½ã€‚
'''
pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)


# ä¸‹ä¸€æ¬¡å°±ä»æœ¬åœ°è·å–ä¿¡æ¯äº†
pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")



''' ================section3: æ¨¡å‹è½¬æ¢==============='''

'''
#Transformers æœ‰ä¸€ä¸ªç‰¹åˆ«é…·çš„åŠŸèƒ½ï¼Œå®ƒèƒ½å¤Ÿä¿å­˜ä¸€ä¸ªæ¨¡å‹ï¼Œ
#å¹¶ä¸”å°†å®ƒåŠ è½½ä¸º PyTorch æˆ– TensorFlow æ¨¡å‹ã€‚
'''
'''
tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)
'''


'''==================section4: è‡ªå®šä¹‰æ¨¡å‹æ„å»º===================='''
''' å¯ä»¥é…ç½®æ¨¡å‹ï¼Œä»¥é€‚åº”è‡ªå·±çš„ä»»åŠ¡ã€‚'''
from transformers import AutoModel
from transformers import AutoConfig

my_config = AutoConfig.from_pretrained(pt_save_directory, n_heads=12)

my_model = AutoModel.from_config(my_config)


'''===================section5: Trainer - PyTorchä¼˜åŒ–è®­ç»ƒå¾ªç¯ '''
from transformers import TrainingArguments
from datasets import load_dataset
from transformers import DataCollatorWithPadding
from transformers import Trainer

''' model'''

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")

'''TrainingArguments å«æœ‰ä½ å¯ä»¥ä¿®æ”¹çš„æ¨¡å‹è¶…å‚æ•°'''
#æ¯”å¦‚å­¦ä¹ ç‡ï¼Œ
#æ‰¹æ¬¡å¤§å°
#è®­ç»ƒæ—¶çš„è¿­ä»£æ¬¡æ•°ã€‚
#å¦‚æœä½ æ²¡æœ‰æŒ‡å®šè®­ç»ƒå‚æ•°ï¼Œé‚£ä¹ˆå®ƒä¼šä½¿ç”¨é»˜è®¤å€¼ï¼š

training_args = TrainingArguments(
    output_dir="path/to/save/folder/", # è¿™é‡Œå­˜çš„æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡å‹çš„æ£€æŸ¥ç‚¹ã€æ—¥å¿—å’Œå…¶ä»–è¾“å‡ºæ–‡ä»¶
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
)

'''åˆ†è¯å™¨'''
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

'''æ•°æ®é›†
æ¯æ¬¡éƒ½ä¸‹è½½ä¸€æ¬¡çš„åŸå› æ˜¯å› ä¸ºé»˜è®¤æƒ…å†µä¸‹ï¼Œ`load_dataset` å‡½æ•°ä¼šä» Hugging Face Hub ä¸‹è½½æ•°æ®é›†ã€‚
ä¸ºäº†é¿å…æ¯æ¬¡éƒ½ä¸‹è½½ï¼Œå¯ä»¥å°†æ•°æ®é›†ç¼“å­˜åˆ°æœ¬åœ°ã€‚
-ä½¿ç”¨ `cache_dir` å‚æ•°æŒ‡å®šç¼“å­˜ç›®å½•
'''


dataset = load_dataset("rotten_tomatoes", cache_dir="./datasets_cache")

'''æ•´ä½“æ•°æ®é›†ï¼Œåˆ†è¯,è¿”å›å€¼è¿˜æ˜¯dataset'''
def tokenize_dataset(dataset):
    return tokenizer(dataset["text"])
# mapæ–¹æ³•éœ€è¦ä¸¤ä¸ªå‚æ•°ï¼šç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯å¯é€‰çš„batchedå‚æ•°ï¼Œé»˜è®¤ä¸ºFalseã€‚
# å‡½æ•°å‚æ•°ç”¨äºå¯¹æ•°æ®é›†ä¸­çš„æ¯ä¸ªå…ƒç´ è¿›è¡Œè½¬æ¢ã€‚
# batchedå‚æ•°å¦‚æœè®¾ç½®ä¸ºTrueï¼Œåˆ™ä¼šå¯¹æ•´ä¸ªæ‰¹æ¬¡çš„æ•°æ®è¿›è¡Œè½¬æ¢ï¼Œè€Œä¸æ˜¯é€ä¸ªå…ƒç´ è¿›è¡Œè½¬æ¢ã€‚
dataset = dataset.map(tokenize_dataset, batched=True)

'''æ•°æ®æ‰¹æ¬¡å¤„ç†å™¨'''


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

''' end: è®­ç»ƒå™¨ '''
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)  # doctest: +SKIP
trainer.train()